---
title: "lab6"
author: "Sarah Culhane"
format: html
execute:
 echo: true
---

## Question 1

```{r}
#load neccessary libraries
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)

#download all data and PDF
root  <- 'https://gdex.ucar.edu/dataset/camels/file'
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')
walk2(remote_files, local_files, download.file, quiet = TRUE)
# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 
camels <- power_full_join(camels ,by = 'gauge_id')
```

First, I downloaded all the necessary data into my data directory. From the documentation PDF, I was able to figure out that zero_q_freq represents the percentage of days within a given period during which the stream flow (Q) is zero, indicating no flow conditions.

# Question 2

```{r}

# load neccessary library(ggplot2)
library(ggthemes)
library(patchwork)

# create map colored by aridity
map_aridity <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = aridity)) +
  scale_color_gradient(low = "yellow", high = "red") +
  labs(title = "map by aridity") +
  ggthemes::theme_map()

# create map colored by mean precipitation 
map_p_mean <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = p_mean)) +
  scale_color_gradient(low = "beige", high = "navy") +
  labs(title = "map by mean precipitation") +
  ggthemes::theme_map()

# Combine the two maps side by side using patchwork
map_aridity + map_p_mean

```

I used the code above to make 2 maps of the sites, colored by aridity and the p_mean column.

# Question 3

```{r}
#run code so far from lab instructions
set.seed(123)
# Bad form to perform simple transformations on the outcome variable within a 
# recipe. So, we'll do it here.
camels <- camels |> 
  mutate(logQmean = log(q_mean))

# Generate the split
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)

#preprocess recipe

# Create a recipe to preprocess the data
rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())

# Prepare the data
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

# Interaction with lm

#  Base lm sets interaction terms with the * symbol
lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)
summary(lm_base)

# Sanity Interaction term from recipe ... these should be equal!!
summary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))

test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)

# Define model
lm_model <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
lm_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(lm_model) %>%
  # Fit the model to the training data
  fit(data = camels_train) 

# Extract the model coefficients from the workflow
summary(extract_fit_engine(lm_wf))$coefficients

library(baguette)
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(rf_model) %>%
  # Fit the model
  fit(data = camels_train) 

# part 2 question code

#make a xboost(engine) regression (mode) model using boost_tree
xgb_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")
#Build a neural network model using the nnet engine from the baguette package using the bag_mlp function
nn_model <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")
#work it into workflow from lab instructions
wf <- workflow_set(
  list(rec),
  list(lm_model, rf_model, xgb_model, nn_model)
) %>%
  workflow_map('fit_resamples', resamples = camels_cv)
#evaluate results
autoplot(wf)
rank_results(wf, rank_metric = "rsq", select_best = TRUE)


```

Looking at the workflow rank result, it looks like neutral network outperformed the linear_reg model and the rand_forest model while the xgboost model underperformed. Going forward, I would use the neutral network model since it has the highest R squared

# Question 4

part 1: Data Splitting

```{r}
camels |> 
  select(aridity, p_mean, elev_mean) |> 
  drop_na()
set.seed(123) #set seed for reproducibility 
camels <- camels |> 
  mutate(logQmean = log(q_mean))
split <- initial_split(camels, prop = 0.75) #Create an initial split with 75% used for training and 25% for testing
camels_train <- training(split)
camels_test <- testing(split) # extract training and testing sets
camels_cv <- vfold_cv(camels_train, v = 10) # build a 10-fold CV dataset as well
```

part 2: Recipe

```{r}
formula <- logQmean ~ aridity + p_mean + elev_mean  
rec <- recipe(formula, data = camels_train) %>% # build a recipe
  step_naomit(all_predictors(), all_outcomes()) %>%
  step_normalize(all_predictors())
```

I used this formula because predictors like aridity, precipitation, and elevation generally play a significant role in stream flow prediction.

part 3: define 3 models

```{r}
# create a random forest model
rf_model <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("regression")
# create a neural network model
nn_model <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")
# create a XGBoost Model
xgb_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")
```

Step 4: create a workflow set

```{r}
model_list <- list(rf = rf_model, xgb = xgb_model, nn = nn_model)
wf_set <- workflow_set( #create a workflow object
  preproc = list(rec), #add recipe
  models = model_list #add models
)

# Fit resamples
set.seed(123)
wf_fit <- workflow_map(wf_set, "fit_resamples", resamples = camels_cv)
```

step 5: evaluation

```{r}
autoplot(wf_fit) # use autoplot

rank_results(wf_fit, rank_metric = "rsq", select_best = TRUE) # use rank_results to compare the models

```

I think the rand_forest model would be best because it has the highest R-squared value, suggesting the strongest model fit.

step 6: exact and evaluate

```{r}
best_model <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rec) %>%
  fit(data = camels_train)

# Make predictions
test_preds <- augment(best_model, new_data = camels_test)

# Evaluate
metrics(test_preds, truth = logQmean, estimate = .pred)

#visualization
library(ggplot2)

ggplot(test_preds, aes(x = logQmean, y = .pred, color = aridity)) +
  geom_point() +
  geom_abline(lty = 2, color = "gray") +
  scale_color_viridis_c() +
  labs(
    title = "Observed vs Predicted Streamflow",
    x = "Observed logQmean",
    y = "Predicted logQmean",
    color = "Aridity Index"
  ) +
  theme_minimal()


```

The results make sense to me, and based on the table the predictions are generally pretty accurate to observed stream flow, which is pretty neat. I also see that more arid regions tend to have a lower overall stream flow and are harder to predict. I based this assumption off the fact that the data is more spread out as the aridity index decreases.
