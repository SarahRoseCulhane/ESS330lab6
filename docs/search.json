[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "lab6",
    "section": "",
    "text": "#load neccessary libraries\nlibrary(tidyverse)\n\nWarning: package 'readr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\nWarning: package 'workflowsets' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\n#download all data and PDF\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\nFirst, I downloaded all the necessary data into my data directory. From the documentation PDF, I was able to figure out that zero_q_freq represents the percentage of days within a given period during which the stream flow (Q) is zero, indicating no flow conditions."
  },
  {
    "objectID": "lab6.html#question-1",
    "href": "lab6.html#question-1",
    "title": "lab6",
    "section": "",
    "text": "#load neccessary libraries\nlibrary(tidyverse)\n\nWarning: package 'readr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\nWarning: package 'workflowsets' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\n#download all data and PDF\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\nFirst, I downloaded all the necessary data into my data directory. From the documentation PDF, I was able to figure out that zero_q_freq represents the percentage of days within a given period during which the stream flow (Q) is zero, indicating no flow conditions."
  },
  {
    "objectID": "ESS330lab8.html",
    "href": "ESS330lab8.html",
    "title": "hyperparameter-tuning",
    "section": "",
    "text": "Lab 8: Machine Learning\nFirst, I loaded the necessary libraries\n\nlibrary(tidyverse)\n\nWarning: package 'readr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\nWarning: package 'workflowsets' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(tidyverse)\nlibrary(naniar)\n\nWarning: package 'naniar' was built under R version 4.4.3\n\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\n\nData Import/Tidy/Transform\nFirst, I read in the data using map, map, read_delim() and powerjoin::power_full_join().\n\n#download all data and PDF\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\nThen, I cleaned and explored the data to ensure that it is a good fit for modeling.\n\n# Initial cleaning: remove NA rows and filter out incomplete cases\ncamels_clean &lt;- camels %&gt;%\n  drop_na(q_mean)\n# remove remaining missing values\ncamels_clean &lt;- camels_clean %&gt;%\n  drop_na()\n# Quick visualization of q_mean\nggdensity(camels_clean$q_mean, \n          main = \"Density plot of Mean Streamflow (q_mean)\", \n          xlab = \"q_mean\")\n\n\n\n\n\n\n\n\n\nData Splitting\nThen, I split the data into a training and testing set\n\nset.seed(123)\ncamels_clean &lt;- camels_clean %&gt;%\n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels_clean, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test &lt;- testing(camels_split)\n\n\n\nFeature Engineering\nThen, I created a recipe\n\nlibrary(tidymodels)\n\nrec &lt;- recipe(q_mean ~ ., data = camels_clean) %&gt;%\n  step_rm(gauge_lat, gauge_lon) %&gt;%                   # Remove as predictors\n  step_novel(all_nominal_predictors()) %&gt;%            # Handle unseen levels\n  step_dummy(all_nominal_predictors()) %&gt;%            # Convert factors to dummies\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())            # Scale numeric features\n\n\n\nResampling and Model Testing\n\nBuild resamples\n\nset.seed(123)\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nBuild Candidate Models\n\nlm_model &lt;- linear_reg(penalty = 0.1, mixture = 0) %&gt;%\n  set_engine(\"glmnet\")\n\nrf_model &lt;- rand_forest(mtry = 5, trees = 500) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_model &lt;- boost_tree(trees = 500, learn_rate = 0.1) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\n\n\nTest the models\n\n\nmodels &lt;- list(\n  \"Linear Regression\" = lm_model,\n  \"Random Forest\" = rf_model,\n  \"XGBoost\" = xgb_model\n)\n\nwf_set &lt;- workflow_set(\n  preproc = list(\"recipe\" = rec),\n  models = models\n)\n\nwf_results &lt;- wf_set %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv)\n\nWarning: package 'glmnet' was built under R version 4.4.3\n\n\nWarning: package 'ranger' was built under R version 4.4.3\n\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\nautoplot(wf_results) #use auotplot to visualize the results of the workflow set\n\n\n\n\n\n\n\n\n\nrank_results(wf_results, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id         .config .metric   mean std_err     n preprocessor model  rank\n  &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_XGBoost   Prepro… rmse    0.0805 1.91e-2    10 recipe       boos…     1\n2 recipe_XGBoost   Prepro… rsq     0.998  7.53e-4    10 recipe       boos…     1\n3 recipe_Random F… Prepro… rmse    0.804  5.51e-2    10 recipe       rand…     2\n4 recipe_Random F… Prepro… rsq     0.910  1.21e-2    10 recipe       rand…     2\n5 recipe_Linear R… Prepro… rmse    1.00   6.50e-2    10 recipe       line…     3\n6 recipe_Linear R… Prepro… rsq     0.834  1.30e-2    10 recipe       line…     3\n\n\n4. Model Selection\nBased on the visualized metrics, I am going to use the XGBoost model. This model outperformed the other models, ranking as the number one model when using the rank_results() function and the autoplot function. Looking at it, it also had the highest rsq. This model is a boosted trees model type with a xgboost engine. The mode used was regression. This model is performing well for this problem because it is good at handeling non-linear relationships, and streamflow (q_mean) is influenced by complex, variable predictors. It’s also a pretty flexible model.\n\n\nModel Tuning\n\nBuild a model for chosen specification\n\nxgb_tuned_model &lt;- \n  boost_tree(\n    tree_depth = tune(),\n    learn_rate = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nCreate a workflow\n\nxgb_workflow &lt;- workflow() %&gt;%\n  add_model(xgb_tuned_model) %&gt;%\n  add_recipe(rec)\n\nI created a workflow\nCheck the Tunable Values/Ranges\nfirst, I extracted the tunable parameters. Then I used the dials$object slot to see the tunable parameters and their ranges\n\ndials &lt;- extract_parameter_set_dials(xgb_workflow) \ndials$object\n\n[[1]]\nTree Depth (quantitative)\nRange: [1, 15]\n\n[[2]]\nLearning Rate (quantitative)\nTransformer: log-10 [1e-100, Inf]\nRange (transformed scale): [-3, -0.5]\n\n\n\n\n\n4. Define the Search Space\n\nlibrary(finetune)  # If not loaded already\n\nWarning: package 'finetune' was built under R version 4.4.3\n\nmy.grid &lt;- grid_latin_hypercube(dials, size = 25)\n\nWarning: `grid_latin_hypercube()` was deprecated in dials 1.3.0.\nℹ Please use `grid_space_filling()` instead.\n\n\n\n\n5. Tune the model\n\nmodel_params &lt;- tune_grid(\n  xgb_workflow,\n  resamples = camels_cv,\n  grid = my.grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\nautoplot(model_params)\n\n\n\n\n\n\n\n\nBased on this, I see that\n\n\n6. check the skills of the tuned model\n\ncollect_metrics(model_params)\n\n# A tibble: 75 × 8\n   tree_depth learn_rate .metric .estimator   mean     n std_err .config        \n        &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n 1         12    0.00728 mae     standard   0.988     10 0.0569  Preprocessor1_…\n 2         12    0.00728 rmse    standard   1.68      10 0.100   Preprocessor1_…\n 3         12    0.00728 rsq     standard   0.976     10 0.00470 Preprocessor1_…\n 4          5    0.00833 mae     standard   0.973     10 0.0561  Preprocessor1_…\n 5          5    0.00833 rmse    standard   1.66      10 0.0990  Preprocessor1_…\n 6          5    0.00833 rsq     standard   0.976     10 0.00457 Preprocessor1_…\n 7          4    0.0789  mae     standard   0.341     10 0.0196  Preprocessor1_…\n 8          4    0.0789  rmse    standard   0.613     10 0.0436  Preprocessor1_…\n 9          4    0.0789  rsq     standard   0.993     10 0.00336 Preprocessor1_…\n10         13    0.179   mae     standard   0.0706    10 0.00725 Preprocessor1_…\n# ℹ 65 more rows\n\n\nBased off what I see,\n\nshow_best(model_params, metric = \"mae\")\n\n# A tibble: 5 × 8\n  tree_depth learn_rate .metric .estimator   mean     n std_err .config         \n       &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1         14      0.303 mae     standard   0.0355    10 0.00321 Preprocessor1_M…\n2         14      0.241 mae     standard   0.0373    10 0.00395 Preprocessor1_M…\n3         13      0.179 mae     standard   0.0706    10 0.00725 Preprocessor1_M…\n4          2      0.134 mae     standard   0.189     10 0.00952 Preprocessor1_M…\n5          8      0.114 mae     standard   0.198     10 0.0126  Preprocessor1_M…\n\n\n\n\n7. Finalize your model\n\nbest_mae &lt;- select_best(model_params, metric = \"mae\")\nfinal_wf &lt;- finalize_workflow( xgb_workflow, best_mae)\n\n\n\n8. Final Model Verification\n\nfinal_fit &lt;- last_fit(final_wf, split = camels_split) # fit the model using last_fit()\ncollect_metrics(final_fit) #evaluate model using collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard      0.0733 Preprocessor1_Model1\n2 rsq     standard      0.998  Preprocessor1_Model1\n\n\nBased on the final model fit, the final model performed better than the training data\n\ntest_predictions &lt;- collect_predictions(final_fit) #Use the collect_predictions() function to check the predictions of the final model on the test data\n\n\nlibrary(ggplot2)\n\nggplot(test_predictions, aes(x = q_mean, y = .pred)) +\n  geom_point(color = \"#1f77b4\", alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#ff7f0e\", linetype = \"dashed\") +\n  geom_abline(slope = 1, intercept = 0, color = \"gray30\", linetype = \"dotted\") +\n  labs(\n    title = \"Predicted vs Actual q_mean on Test Set\",\n    x = \"Actual q_mean\",\n    y = \"Predicted q_mean\"\n  ) +\n  theme_minimal() +\n  scale_color_viridis_d()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nBuilding a map\n\nfinal_model_fit &lt;- fit(final_wf, data = camels_clean) #fit the final model to the full data set\nlibrary(broom)\npredicted_all &lt;- augment(final_model_fit, new_data = camels_clean) # Make predictions across full data using augment()\npredicted_all &lt;- predicted_all %&gt;% #calculate residuals\n  mutate(residual = (.pred - q_mean)^2) \nlibrary(ggplot2)\n\nNow, I will create maps\n\nlibrary(ggplot2)\n\nmap_pred &lt;- ggplot(predicted_all, aes(x = gauge_lon, y = gauge_lat)) +\n  geom_point(aes(color = .pred), size = 2, alpha = 0.8) +\n  scale_color_viridis_c(name = \"Predicted q_mean\") +\n  coord_fixed(1.3) +\n  labs(title = \"Predicted Mean Streamflow\") +\n  theme_minimal()\nmap_resid &lt;- ggplot(predicted_all, aes(x = gauge_lon, y = gauge_lat)) +\n  geom_point(aes(color = residual), size = 2, alpha = 0.8) +\n  scale_color_viridis_c(name = \"Residual (squared)\") +\n  coord_fixed(1.3) +\n  labs(title = \"Residuals of Predictions\") +\n  theme_minimal()\nlibrary(patchwork)\n\nmap_pred + map_resid + plot_annotation(title = \"Model Predictions and Residuals Across CONUS\")"
  }
]