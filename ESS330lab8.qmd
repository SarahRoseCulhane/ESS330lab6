---
title: "hyperparameter-tuning"
author: "Sarah Culhane"
editor: visual
---

Lab 8: Machine Learning

First, I loaded the necessary libraries

```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(tidyverse)
library(naniar)
library(ggpubr)

```

**Data Import/Tidy/Transform**

First, I read in the data using map, `map`, `read_delim()` and `powerjoin::power_full_join().`

```{r}
#download all data and PDF
root  <- 'https://gdex.ucar.edu/dataset/camels/file'
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')
walk2(remote_files, local_files, download.file, quiet = TRUE)
# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 
camels <- power_full_join(camels ,by = 'gauge_id')
```

Then, I cleaned and explored the data to ensure that it is a good fit for modeling.

```{r}
# Initial cleaning: remove NA rows and filter out incomplete cases
camels_clean <- camels %>%
  drop_na(q_mean)
# remove remaining missing values
camels_clean <- camels_clean %>%
  drop_na()
# keep guage_lat and guage_lon for later
camels_clean <- camels_clean %>%
  select(q_mean, everything(), gauge_lat, gauge_lon)

# Quick visualization of q_mean
ggdensity(camels_clean$q_mean, 
          main = "Density plot of Mean Streamflow (q_mean)", 
          xlab = "q_mean")
```

# Data Splitting

Then, I split the data into a training and testing set

```{r}
set.seed(123)
camels_clean <- camels_clean 

camels_split <- initial_split(camels_clean, prop = 0.8)
camels_train <- training(camels_split)
camels_test <- testing(camels_split)
```

# Feature Engineering

Then, I created a recipe using aridity and p_mean as the predictors, cleaning up the data 

```{r}
library(tidymodels)

# Create a recipe to preprocess the data
rec <-  recipe(q_mean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())
```

# Resampling and Model Testing

1.  Build resamples

    ```{r}
    set.seed(123)
    camels_cv <- vfold_cv(camels_train, v = 10)

    ```

2.  Build Candidate Models

    ```{r}
  # Linear regression model
lin_mod <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Random forest model
rf_mod <- rand_forest(trees = 500) %>%
  set_engine("ranger") %>%
  set_mode("regression")

# Boosted tree model
xgb_mod <- boost_tree(trees = 500, learn_rate = 0.1) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

    ```



3.  Test the models

```{r}
# Create a named list of models
model_list <- list(
  linear_regression = lin_mod,
  random_forest = rf_mod,
  boosted_tree = xgb_mod
)

# Create the workflow set
wf_set <- workflow_set(
  preproc = list(rec),
  models = model_list
)

# Fit models across resamples
set.seed(123)
wf_results <- wf_set %>%
  workflow_map(resamples = camels_cv, metrics = metric_set(rmse, rsq), seed = 123)
```
Then, I used autoplot ot visualize the results of the workflow set
```{r}
autoplot(wf_results)

```

4\. Model Selection

Based on the visualized metrics, I am going to use the random forest model. This model outperformed the other models, ranking as the number one model when using the rank_results() function and the autoplot function. Looking at it, it also had the highest rsq. and the lowest rmse, supporting the outputted table. The model I selected is an ensemble learning method model type with multiple engine types. The most popular ones are the randomForest engine and the Ranger engine. The two common modes are classification and regression. For this one, I am using regression.

# Model Tuning

1.  Build a model for chosen specification

    ```{r}
# Define a tunable random forest model
rf_tune_mod <- rand_forest(
  mtry = tune(),
  min_n = tune(),
) %>%
  set_engine("ranger") %>%
  set_mode("regression")


    ```

2.  Create a workflow

    ```{r}
  # Build a workflow with the tunable random forest model and the recipe
rf_tune_workflow <- workflow() %>%
  add_model(rf_tune_mod) %>%
  add_recipe(rec)

    ```

    I created a workflow

3.  Check the Tunable Values/Ranges

    first, I extracted the tunable parameters. Then I used the dials\$object slot to see the tunable parameters and their ranges

    ```{r}
# Extract tunable parameter set from the workflow
dials <- extract_parameter_set_dials(rf_tune_workflow)

# View the parameters and their default ranges
dials$object 

    ```

#    4. Define the Search Space

```{r}
# Finalize the mtry and min_n parameters
rf_params <- parameters(
  mtry(),
  min_n()
)

# Finalize the parameter grid based on the dataset (using the data from training set)
rf_params <- finalize(rf_params, training(camels_split))  # Replace camels_split with your training data

# Generate a space-filling grid of hyperparameters (size = 25 combinations)
grid_values <- grid_space_filling(
  rf_params,
  size = 25
)


```

# 5. Tune the model

```{r}
model_params <-  tune_grid(
    rf_tune_workflow,
    resamples = camels_cv,
    grid = my.grid,
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE)
  )

autoplot(model_params)

```

Based on this, I see that

# 6. check the skills of the tuned model
```{r}
# Collect all metrics from the tuned model
model_metrics <- collect_metrics(model_params)

# View the collected metrics
model_metrics

```
I used the code above to collect_metrics() function to check the skill of the tuned model.Based off what I see,

```{r}
# Show the best model based on MAE
show_best(model_params, metric = "mae")

```
I used the code above to Use the show_best() function to show the best performing model based on Mean Absolute Error.
```{r}
# Select the best hyperparameter set
hp_best <- select_best(model_params, metric = "mae")

# View the best hyperparameter set
hp_best

```
I used the code above to save the best performing hyperparameter set to an object called hp_best.

# 7. Finalize your model

```{r}
# Finalize the workflow with the best hyperparameters
final_rf_workflow <- finalize_workflow(
  rf_tune_workflow,  # Original workflow
  hp_best            # Best hyperparameters obtained from tune_grid
)

# View the finalized workflow
final_rf_workflow

# Fit the final model on the full training data
final_rf_model <- fit(final_rf_workflow, data = training(camels_split))  # Replace with your actual training data

# View the final fitted model
final_rf_model

```

# 8. Final Model Verification

```{r}
# Fit the final model to the training data and validate it on the test data
final_results <- last_fit(
  final_wf,
  split = camels_split,  # The result from initial_split()
  metrics = metric_set(rmse, rsq, mae)  # Metrics for evaluation
)

```

Based on the final model fit, the final model performed worse than the training model set. It has a lower rsq and higher rsme.

```{r}
# Collect metrics for the final model
final_metrics <- collect_metrics(final_results)

# View the performance on the test set
final_metrics

```
```{r}
# Collect predictions for the final model on the test data
final_predictions <- collect_predictions(final_results)

# View the first few predictions to make sure the object was created
head(final_predictions)

```
# building a map
```{r}
library(ggplot2)

ggplot(final_predictions, aes(x = .pred, y = logQmean)) +
  geom_point(aes(color = logQmean)) +  # Scatter plot of predictions vs actuals
  geom_smooth(method = "lm", se = FALSE, color = "blue") +  # Linear fit
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +  # 1:1 line
  scale_color_viridis_c() +  # Color palette
  labs(
    title = "Predicted vs Actual Values",
    x = "Predicted Values",
    y = "Actual Values"
  ) +
  theme_minimal()

```

# Building a map

```{r}
final_model_fit <- fit(final_wf, data = camels_clean) #fit the final model to the full data set
library(broom)
predicted_all <- augment(final_model_fit, new_data = camels_clean) # Make predictions across full data using augment()
predicted_all <- predicted_all %>% #calculate residuals
  mutate(residual = (.pred - logQmean)^2) 
library(ggplot2)
```

Now, I will create maps

```{r}
library(ggplot2)
library(sf)

# Assuming you have a spatial column 'geometry' in your data
ggplot(predicted_all) +
  geom_sf(aes(fill = .pred)) +  # Color the map based on predictions
  scale_fill_viridis_c() +  # Color palette for fill
  labs(title = "Predicted Values Across CONUS") +
  theme_minimal()

# Add residuals (difference between truth and predictions)
predicted_all <- predicted_all %>%
  mutate(residuals = logQmean - .pred)

ggplot(predicted_all) +
  geom_sf(aes(fill = residuals)) +  # Color the map based on residuals
  scale_fill_viridis_c() +  # Color palette for residuals
  labs(title = "Residuals Across CONUS") +
  theme_minimal()

library(patchwork)

# Create two separate maps
map_predictions <- ggplot(predicted_all) +
  geom_sf(aes(fill = .pred)) + 
  scale_fill_viridis_c() + 
  labs(title = "Predictions Across CONUS") +
  theme_minimal()

map_residuals <- ggplot(predicted_all) +
  geom_sf(aes(fill = residuals)) + 
  scale_fill_viridis_c() + 
  labs(title = "Residuals Across CONUS") +
  theme_minimal()

# Combine the maps
map_predictions + map_residuals


```
