---
title: "hyperparameter-tuning"
author: "Sarah Culhane"
editor: visual
---

Lab 8: Machine Learning

First, I loaded the necessary libraries

```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(tidyverse)
library(naniar)
library(ggpubr)

```

**Data Import/Tidy/Transform**

First, I read in the data using map, `map`, `read_delim()` and `powerjoin::power_full_join().`

```{r}
#download all data and PDF
root  <- 'https://gdex.ucar.edu/dataset/camels/file'
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')
walk2(remote_files, local_files, download.file, quiet = TRUE)
# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 
camels <- power_full_join(camels ,by = 'gauge_id')
```

Then, I cleaned and explored the data to ensure that it is a good fit for modeling.

```{r}
# Initial cleaning: remove NA rows and filter out incomplete cases
camels_clean <- camels %>%
  drop_na(q_mean)
# Skim for EDA
skim(camels_clean)
# remove remaining missing values
camels_clean <- camels_clean %>%
  drop_na()
# Quick visualization of q_mean
ggdensity(camels_clean$q_mean, 
          main = "Density plot of Mean Streamflow (q_mean)", 
          xlab = "q_mean")
```

# Data Splitting

Then, I split the data into a training and testing set

```{r}
set.seed(123)
camels_clean <- camels_clean %>%
  mutate(logQmean = log(q_mean))

camels_split <- initial_split(camels_clean, prop = 0.8)
camels_train <- training(camels_split)
camels_test <- testing(camels_split)
```

# Feature Engineering

Then, I created a recipe

```{r}
library(tidymodels)

rec <- recipe(q_mean ~ ., data = camels_clean) %>%
  step_rm(gauge_lat, gauge_lon) %>%                   # Remove as predictors
  step_novel(all_nominal_predictors()) %>%            # Handle unseen levels
  step_dummy(all_nominal_predictors()) %>%            # Convert factors to dummies
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())            # Scale numeric features

```

# Resampling and Model Testing

1.  Build resamples

    ```{r}
    set.seed(123)
    camels_cv <- vfold_cv(camels_train, v = 10)

    ```

2.  Build Candidate Models

    ```{r}
    lm_model <- linear_reg(penalty = 0.1, mixture = 0) %>%
      set_engine("glmnet")

    rf_model <- rand_forest(mtry = 5, trees = 500) %>%
      set_engine("ranger") %>%
      set_mode("regression")

    xgb_model <- boost_tree(trees = 500, learn_rate = 0.1) %>%
      set_engine("xgboost") %>%
      set_mode("regression")

    ```

<!-- -->

2.  Test the models

```{r}
models <- list(
  "Linear Regression" = lm_model,
  "Random Forest" = rf_model,
  "XGBoost" = xgb_model
)

wf_set <- workflow_set(
  preproc = list("recipe" = rec),
  models = models
)

wf_results <- wf_set %>%
  workflow_map("fit_resamples", resamples = camels_cv)

autoplot(wf_results) #use auotplot to visualize the results of the workflow set
```

```{r}
rank_results(wf_results, rank_metric = "rsq", select_best = TRUE)
```

4\. Model Selection

Based on the visualized metrics, I am going to use the XGBoost model. This model outperformed the other models, ranking as the number one model when using the rank_results() function and the autoplot function. Looking at it, it also had the highest rsq. This model is a boosted trees model type with a xgboost engine. The mode used was regression. This model is performing well for this problem because it is good at handeling non-linear relationships, and streamflow (q_mean) is influenced by complex, variable predictors. It's also a pretty flexible model.

# Model Tuning

1.  Build a model for chosen specification

    ```{r}
    xgb_tuned_model <- 
      boost_tree(
        tree_depth = tune(),
        learn_rate = tune()
      ) %>%
      set_engine("xgboost") %>%
      set_mode("regression")
    ```

2.  Create a workflow

    ```{r}
    xgb_workflow <- workflow() %>%
      add_model(xgb_tuned_model) %>%
      add_recipe(rec)

    ```

    I created a workflow

3.  Check the Tunable Values/Ranges

    first, I extracted the tunable parameters. Then I used the dials\$object slot to see the tunable parameters and their ranges

    ```{r}
    dials <- extract_parameter_set_dials(xgb_workflow) 
    dials$object
    ```

#    4. Define the Search Space

```{r}
library(finetune)  # If not loaded already

my.grid <- grid_latin_hypercube(dials, size = 25)
```

# 5. Tune the model

```{r}
model_params <- tune_grid(
  xgb_workflow,
  resamples = camels_cv,
  grid = my.grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = TRUE)
)

autoplot(model_params)

```

Based on this, I see that

# 6. check the skills of the tuned model

```{r}
collect_metrics(model_params)
```

Based off what I see,

```{r}
show_best(model_params, metric = "mae")

```

# 7. Finalize your model

```{r}
best_mae <- select_best(model_params, metric = "mae")
final_wf <- finalize_workflow( xgb_workflow, best_mae)
```

# 8. Final Model Verification

```{r}
final_fit <- last_fit(final_wf, split = camels_split) # fit the model using last_fit()
collect_metrics(final_fit) #evaluate model using collect_metrics()
```

Based on the final model fit, the final model performed better than the training data

```{r}
test_predictions <- collect_predictions(final_fit) #Use the collect_predictions() function to check the predictions of the final model on the test data
```

```{r}
library(ggplot2)

ggplot(test_predictions, aes(x = q_mean, y = .pred)) +
  geom_point(color = "#1f77b4", alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = "#ff7f0e", linetype = "dashed") +
  geom_abline(slope = 1, intercept = 0, color = "gray30", linetype = "dotted") +
  labs(
    title = "Predicted vs Actual q_mean on Test Set",
    x = "Actual q_mean",
    y = "Predicted q_mean"
  ) +
  theme_minimal() +
  scale_color_viridis_d()

```

# Building a map

```{r}
final_model_fit <- fit(final_wf, data = camels_clean) #fit the final model to the full data set
library(broom)
predicted_all <- augment(final_model_fit, new_data = camels_clean) # Make predictions across full data using augment()
predicted_all <- predicted_all %>% #calculate residuals
  mutate(residual = (.pred - q_mean)^2) 
library(ggplot2)
```

Now, I will create maps

```{r}
library(ggplot2)

map_pred <- ggplot(predicted_all, aes(x = gauge_lon, y = gauge_lat)) +
  geom_point(aes(color = .pred), size = 2, alpha = 0.8) +
  scale_color_viridis_c(name = "Predicted q_mean") +
  coord_fixed(1.3) +
  labs(title = "Predicted Mean Streamflow") +
  theme_minimal()
map_resid <- ggplot(predicted_all, aes(x = gauge_lon, y = gauge_lat)) +
  geom_point(aes(color = residual), size = 2, alpha = 0.8) +
  scale_color_viridis_c(name = "Residual (squared)") +
  coord_fixed(1.3) +
  labs(title = "Residuals of Predictions") +
  theme_minimal()
library(patchwork)

map_pred + map_resid + plot_annotation(title = "Model Predictions and Residuals Across CONUS")

```
