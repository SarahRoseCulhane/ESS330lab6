---
title: "hyperparameter-tuning"
author: "Sarah Culhane"
editor: visual
format: html
execute:
 echo: true
---

Lab 8: Machine Learning

First, I loaded the necessary libraries

```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(tidyverse)
library(naniar)
library(ggpubr)

```

**Data Import/Tidy/Transform**

First, I read in the data using map, `map`, `read_delim()` and `powerjoin::power_full_join().`

```{r}
#download all data and PDF
root  <- 'https://gdex.ucar.edu/dataset/camels/file'
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')
walk2(remote_files, local_files, download.file, quiet = TRUE)
# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 
camels <- power_full_join(camels ,by = 'gauge_id')
```

First, I visualized the data

```{r}
visdat::vis_dat(camels)
```
```{r}
skimr::skim(camels)
```
Then, I cleaned the data and did a quick visualization of q_mean

```{r}
# remove remaining missing values
camels_clean <- camels %>%
  drop_na()
# keep guage_lat and guage_lon for later
camels_clean <- camels_clean %>%
  select(q_mean, everything(), gauge_lat, gauge_lon)

# Quick visualization of q_mean
ggdensity(camels_clean$q_mean, 
          main = "Density plot of Mean Streamflow (q_mean)", 
          xlab = "q_mean")
```

# Data Splitting

Then, I split the data into a training and testing set and built resamples using the vfold_cv() function to generate 10 k-fold samples for cross-validation.

```{r}
set.seed(123)
camels_clean <- camels_clean %>%
  mutate(logQmean = log(q_mean))

camels_split <- initial_split(camels_clean, prop = 0.8)
camels_train <- training(camels_split)
camels_test <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)
```

# Feature Engineering

Then, I created a recipe using aridity and p_mean as the predictors, cleaning up the data 

```{r}
library(tidymodels)

# Create a recipe to preprocess the data
rec <-  recipe(q_mean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())

#Bake Data
baked_data <- prep(rec, camels_train) %>%
  bake(new_data = NULL)
```

# Resampling and Model Testing

1.  Build Candidate Models

    ```{r}
  # Linear regression model
lin_mod <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Random forest model
rf_mod <- rand_forest(trees = 500) %>%
  set_engine("ranger") %>%
  set_mode("regression")

# Boosted tree model
xgb_mod <- boost_tree(trees = 500, learn_rate = 0.1) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

    ```

2. define workflows

```{r}
# Random Forest Workflow
rf_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(rf_mod)

#Boosted tree model
xgb_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model (xgb_mod)

#linear regression model
lin_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lin_mod)
```

Then, I tested the models
```{r}
my_metrics <- metric_set(rmse, rsq, mae)

wf_set <- workflow_set(
  preproc = list(my_recipe = rec),
  models = list(
    xgboost = xgb_mod,
    random_forest = rf_mod,
    linear_reg = lin_mod
  )
)  
    
wf_results <- wf_set %>%
  workflow_map(
    "fit_resamples",
    resamples = camels_cv,
    metrics = my_metrics,
    control = control_resamples(save_pred = TRUE)
  )

autoplot(wf_results)
```

4\. Model Selection

Based on the visualized metrics, I am going to use the random forest model. This model outperformed the other models, ranking as the number one model when using the rank_results() function and the autoplot function. Looking at it, it also had the highest rsq. and the lowest rmse, supporting the outputted table. The model I selected is an ensemble learning method model type with multiple engine types. The most popular ones are the randomForest engine and the Ranger engine. For the purpose of this exercise, I am going to ue the ranger engine. The two common modes are classification and regression. For this one, I am using regression. I think the randomforest model is performing well it is good at handling non-linear relationships, and streamflow probably does not have a linear relationship with predictors like aridity and precipitation

# Model Tuning

1.  Build a model for chosen specification

```{r}
# Define a tunable random forest model
rf_tune_mod <- rand_forest(
  mtry = tune(),
  min_n = tune(),
) %>%
  set_engine("ranger") %>%
  set_mode("regression")
```
I used the code abovee to build a model. The hyperparameters I chose to tune are mtry and min_n. The metry hyperparameters referrs to the number of variables randomly sampled as candidates at each split and the min_n hyperparameter is an integer for the minimum number of data points in a node that are required for the node to be split further.

2.  Create a workflow

```{r}
  # Build a workflow with the tunable random forest model and the recipe
rf_tune_workflow <- workflow() %>%
  add_model(rf_tune_mod) %>%
  add_recipe(rec)
```

I used the code above to create a workflow.

3.  Check the Tunable Values/Ranges

first, I extracted the tunable parameters. Then I used the dials\$object slot to see the tunable parameters and their ranges

```{r}
# Extract tunable parameter set from the workflow
dials <- extract_parameter_set_dials(rf_tune_workflow)

# View the parameters and their default ranges
dials$object 
```
#    4. Define the Search Space

```{r}
# Finalize the mtry and min_n parameters
rf_params <- parameters(
  mtry(range = c(1L, 3L)),
  min_n()
)

# Finalize the parameter grid based on the dataset (using the data from training set)
rf_params <- finalize(rf_params, training(camels_split))  # Replace camels_split with your training data

# Generate a space-filling grid of hyperparameters (size = 25 combinations)
my_grid <- grid_space_filling(rf_params, size = 25)
```
When I tried to use grid_latin_hypercute(), Rstudio gave me the following error message: "Warning: `grid_latin_hypercube()` was deprecated in dials 1.3.0.Please use `grid_space_filling()`" so I used the grid_space_filling function instead. I was also unsure of what to put for dial size. The instructions on the lab said 20 but the rubric said 25 so I just whent with 25.

# 5. Tune the model

```{r}
model_params <-  tune_grid(
    rf_tune_workflow,
    resamples = camels_cv,
    grid = my_grid,
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE)
  )

autoplot(model_params)
```
Based on this, I see that only having 1 randomly selected predictor producer the best result. This makes sense because I only have one predictor in my recipe and did chose the mtry hyperparameter. Now I know that was maybe not the best hyperparameter to go with but at this point I am going to stick with it. As for the minimal mode size, I see that as the number of nodes increase, the rsq goes down and the rmse and mae goes up, which tells me that the best node size is going to be somewhere in the lower range.

# 6. check the skills of the tuned model
```{r}
# Collect all metrics from the tuned model
model_metrics <- collect_metrics(model_params)

# View the collected metrics
model_metrics
```
When I used the collect_metrics() function, I see a 54x8 tibble with different combonations of mtry and min_n, the hyperparameters I chose to tune for my model. I also see the different metrics; mae, rsq, and rmse, and the resulting statistics. The only value for mtry I am getting is one, which, again, makes sense because my recipe did only use one predictor. 

```{r}
# Show the best model based on MAE
show_best(model_params, metric = "mae")

```
I used the code above to Use the show_best() function to show the best performing hyperparameter set for my model based on Mean Absolute Error. The best performing hyperparameter set, based on this table, is the one with a mtry of 1 and a min_n of 11.

```{r}
# Select the best hyperparameter set
hp_best <- select_best(model_params, metric = "mae")

# View the best hyperparameter set
hp_best

```
I used the code above to save the best performing hyperparameter set to an object called hp_best.It looks like the best model has a mtry of one ad a min_n of 13

# 7. Finalize your model

```{r}
# Finalize the workflow with the best hyperparameters
final_rf_workflow <- finalize_workflow(
  rf_tune_workflow,  # Original workflow
  hp_best            # Best hyperparameters obtained from tune_grid
)

# View the finalized workflow
final_rf_workflow

# Fit the final model on the full training data
final_rf_model <- fit(final_rf_workflow, data = training(camels_split))  # Replace with your actual training data

# View the final fitted model
final_rf_model

```
I ran finalize_workflow() to create a final workflow object

# 8. Final Model Verification

```{r}
# use last_fit() to fit the final model to the training data and validate it on the test data
final_results <- last_fit(
  final_rf_workflow,
  split = camels_split, 
  metrics = metric_set(rmse, rsq, mae)  # Metrics for evaluation
)

```

```{r}
# Collect metrics for the final model
final_metrics <- collect_metrics(final_results)

# View the performance on the test set
final_metrics

```
Based on the final model fit, the final model performed worse than the training model set. It has a slightly lower rsq and higher rsme.

```{r}
# Collect predictions for the final model on the test data
final_predictions <- collect_predictions(final_results)

# View the first few predictions to make sure the object was created
head(final_predictions)

final_fit_ful <- fit(final_rf_workflow, data = camels_clean)
```
Looking at this table, the .pred object was created successfully.

# plot predicted vs actual
```{r}
library(ggplot2)

ggplot(final_predictions, aes(x = .pred, y = q_mean)) +
  geom_point(color = "red", alpha = 0.6) +
  geom_smooth(method = "lm", color = "blue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "predicted vs actual values",
    x = "predicted",
    y = "actual"
  ) +
  theme_minimal()

```

Now, I will two different maps. One will be of predicted q_mean and the other will be of residuals.

```{r}
library(ggplot2)
library(sf)

predictions_all <- augment(final_fit_ful, new_data = camels_clean)

predictions_all <- predictions_all %>%
  mutate(residual = (q_mean -.pred))

pred_map <- ggplot(predictions_all, aes(x = gauge_lon, y = gauge_lat, color = .pred)) +
  geom_point(size = 2) +
  scale_color_viridis_c(name = "predicted") +
  labs(title = "predicted q_mean") +
  coord_fixed() +
  theme_minimal()

resid_map <- ggplot(predictions_all, aes(x = gauge_lon, y = gauge_lat, color = residual)) +
  geom_point(size = 2) +
  scale_color_viridis_c(name = "Residuals") +
  labs(title = "residuals") +
  coord_fixed() +
  theme_minimal()
library(patchwork)

pred_map + resid_map
  

```
