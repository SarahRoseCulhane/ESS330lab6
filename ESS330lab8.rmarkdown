---
title: "hyperparameter-tuning"
author: "Sarah Culhane"
editor: visual
---



Lab 8: Machine Learning

First, I loaded the necessary libraries



```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(tidyverse)
library(naniar)
library(ggpubr)

```



**Data Import/Tidy/Transform**

First, I read in the data using map, `map`, `read_delim()` and `powerjoin::power_full_join().`



```{r}
#download all data and PDF
root  <- 'https://gdex.ucar.edu/dataset/camels/file'
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')
walk2(remote_files, local_files, download.file, quiet = TRUE)
# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 
camels <- power_full_join(camels ,by = 'gauge_id')
```



Then, I cleaned and explored the data to ensure that it is a good fit for modeling.



```{r}
# Initial cleaning: remove NA rows and filter out incomplete cases
camels_clean <- camels %>%
  drop_na(q_mean)
# remove remaining missing values
camels_clean <- camels_clean %>%
  drop_na()
# Quick visualization of q_mean
ggdensity(camels_clean$q_mean, 
          main = "Density plot of Mean Streamflow (q_mean)", 
          xlab = "q_mean")
```



# Data Splitting

Then, I split the data into a training and testing set



```{r}
set.seed(123)
camels_clean <- camels_clean %>%
  mutate(logQmean = log(q_mean))

camels_split <- initial_split(camels_clean, prop = 0.8)
camels_train <- training(camels_split)
camels_test <- testing(camels_split)
```



# Feature Engineering

Then, I created a recipe



```{r}
library(tidymodels)

rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())

# Prepare the data
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)
```



# Resampling and Model Testing

1.  Build resamples



    ```{r}
    set.seed(123)
    camels_cv <- vfold_cv(camels_train, v = 10)

    ```



2.  Build Candidate Models



    ```{r}
    lm_model <- linear_reg(penalty = 0.1, mixture = 0) %>%
      set_engine("glmnet")

    rf_model <- rand_forest(mtry = 5, trees = 500) %>%
      set_engine("ranger") %>%
      set_mode("regression")

    xgb_model <- boost_tree(trees = 500, learn_rate = 0.1) %>%
      set_engine("xgboost") %>%
      set_mode("regression")

    ```



<!-- -->

2.  Test the models



```{r}
models <- list(
  "Linear Regression" = lm_model,
  "Random Forest" = rf_model,
  "XGBoost" = xgb_model
)

wf_set <- workflow_set(
  preproc = list("recipe" = rec),
  models = models
)

wf_results <- wf_set %>%
  workflow_map("fit_resamples", resamples = camels_cv)

autoplot(wf_results) #use auotplot to visualize the results of the workflow set
```

```{r}
rank_results(wf_results, rank_metric = "rsq", select_best = TRUE)
```



4\. Model Selection

Based on the visualized metrics, I am going to use the random forest model. This model outperformed the other models, ranking as the number one model when using the rank_results() function and the autoplot function. Looking at it, it also had the highest rsq. and the lowest rmse, supporting the outputted table. 

# Model Tuning

1.  Build a model for chosen specification



    ```{r}
library(tidymodels)

rf_model_tune <- rand_forest(
  mtry = tune(),      # Number of predictors to try at each split
  min_n = tune(),     # Minimum node size before a split
  trees = 500         # Fixed number of trees
) %>%
  set_engine("ranger") %>%
  set_mode("regression")

    ```



2.  Create a workflow



    ```{r}
   wf_tune <- workflow() %>%
  add_model(rf_model_tune) %>%
  add_recipe(rec)   # Replace with your actual recipe object name

    ```



    I created a workflow

3.  Check the Tunable Values/Ranges

    first, I extracted the tunable parameters. Then I used the dials\$object slot to see the tunable parameters and their ranges



    ```{r}
  dials <- extract_parameter_set_dials(wf_tune)
dials$object  # View the ranges and types of each parameter

    ```



#    4. Define the Search Space



```{r}
# Assuming your outcome is q_mean
final_dials <- dials %>%
  finalize(select(camels_train, q_mean))

set.seed(123)
my.grid <- grid_latin_hypercube(
  final_dials,
  size = 20  # or 20 as per instructions
)
```



# 5. Tune the model



```{r}
model_params <- tune_grid(
  wf_tune,
  resamples = camels_cv,  # Your 10-fold CV object
  grid = my.grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = TRUE)
)

autoplot(model_params)  # Visualize model performance across the grid


```



Based on this, I see that

# 6. check the skills of the tuned model



```{r}
collect_metrics(model_params)
```



Based off what I see,



```{r}
show_best(model_params, metric = "mae")

```



# 7. Finalize your model



```{r}
best_mae <- select_best(model_params, metric = "mae")
final_wf <- finalize_workflow(wf_tune, best_mae)
```



# 8. Final Model Verification



```{r}
# Fit the final model to the training data and validate it on the test data
final_results <- last_fit(
  final_wf,
  split = camels_split,  # The result from initial_split()
  metrics = metric_set(rmse, rsq, mae)  # Metrics for evaluation
)

```



Based on the final model fit, the final model performed worse than the training model set. It has a lower rsq and higher rsme.



```{r}
# Collect metrics for the final model
final_metrics <- collect_metrics(final_results)

# View the performance on the test set
final_metrics

```

```{r}
# Collect predictions for the final model on the test data
final_predictions <- collect_predictions(final_results)

# View the first few predictions to make sure the object was created
head(final_predictions)

```


# building a map


```{r}
library(ggplot2)

ggplot(final_predictions, aes(x = .pred, y = logQmean)) +
  geom_point(aes(color = logQmean)) +  # Scatter plot of predictions vs actuals
  geom_smooth(method = "lm", se = FALSE, color = "blue") +  # Linear fit
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +  # 1:1 line
  scale_color_viridis_c() +  # Color palette
  labs(
    title = "Predicted vs Actual Values",
    x = "Predicted Values",
    y = "Actual Values"
  ) +
  theme_minimal()

```



# Building a map



```{r}
final_model_fit <- fit(final_wf, data = camels_clean) #fit the final model to the full data set
library(broom)
predicted_all <- augment(final_model_fit, new_data = camels_clean) # Make predictions across full data using augment()
predicted_all <- predicted_all %>% #calculate residuals
  mutate(residual = (.pred - logQmean)^2) 
library(ggplot2)
```



Now, I will create maps



```{r}
library(ggplot2)
library(sf)

# Assuming you have a spatial column 'geometry' in your data
ggplot(predicted_all) +
  geom_sf(aes(fill = .pred)) +  # Color the map based on predictions
  scale_fill_viridis_c() +  # Color palette for fill
  labs(title = "Predicted Values Across CONUS") +
  theme_minimal()

# Add residuals (difference between truth and predictions)
predicted_all <- predicted_all %>%
  mutate(residuals = logQmean - .pred)

ggplot(predicted_all) +
  geom_sf(aes(fill = residuals)) +  # Color the map based on residuals
  scale_fill_viridis_c() +  # Color palette for residuals
  labs(title = "Residuals Across CONUS") +
  theme_minimal()

library(patchwork)

# Create two separate maps
map_predictions <- ggplot(predicted_all) +
  geom_sf(aes(fill = .pred)) + 
  scale_fill_viridis_c() + 
  labs(title = "Predictions Across CONUS") +
  theme_minimal()

map_residuals <- ggplot(predicted_all) +
  geom_sf(aes(fill = residuals)) + 
  scale_fill_viridis_c() + 
  labs(title = "Residuals Across CONUS") +
  theme_minimal()

# Combine the maps
map_predictions + map_residuals


```

